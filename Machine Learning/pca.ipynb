{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/saskeli/x/blob/master/pca.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n",
    "\n",
    "|                                     -                                     |\n",
    "|---------------------------------------------------------------------------|\n",
    "| [Exercise 8 (explained variance)](<#Exercise-8-(explained-variance&#41;>) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML: Principal component analysis\n",
    "\n",
    "## Principal component analysis\n",
    "\n",
    "Principal component analysis is an unsupervised learning method that tries to detect the directions in which the vector formed data varies most. It first finds the direction of highest variance, and then proceeds to discover directions of highest variance that are orthogonal to those direction already found. So, for n dimensional data, it returns, by default, n orthogonal directions and the corresponding variances. These directions are called *pricipal axes*, and if we project a data point to these axes, we get the *principal components* of each axis.\n",
    "\n",
    "To use another terminology, the set of principal axes forms a base for the vector space where the data points reside, and the principal components are the coordinates of the data points in this new coordinate system. The `PCA` class in the scikit-learn library has a `transform` method, which transforms data to this new coordinate system.\n",
    "\n",
    "Let's look at an example where the data is from multi-variate Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div class=\"alert alert-info\">Exercise 8 (explained variance)</div>\n",
    "\n",
    "This exercise can give two points at maximum!\n",
    "\n",
    "Part 1.\n",
    "\n",
    "Write function `explained_variance` which reads the tab separated file \"data.tsv\". The data contains 10 features. Then fit PCA to the data. The function should return two lists (or 1D arrays). The first list should contain the variances of all the features.  The second list should consist of the explained variances returned by the PCA.\n",
    "\n",
    "In the main function print these values in the following form:\n",
    "```\n",
    "The variances are: ?.??? ?.??? ...\n",
    "The explained variances after PCA are: ?.??? ?.??? ...\n",
    "```\n",
    "Print the values with three decimal precision and separate the values by a space.\n",
    "\n",
    "Part 2.\n",
    "\n",
    "Plot the cumulative explained variances. The y-axis should be the cumulative sum, and the x-axis the number of terms in the cumulative sum.\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explained_variance():\n",
    "    df = pd.read_csv(\"src/data.tsv\", sep= \"\\t\")\n",
    "    model = PCA()\n",
    "    model.fit(df)\n",
    "    return df.var(axis=0), model.explained_variance_\n",
    "\n",
    "def main():\n",
    "    v, ev = explained_variance()\n",
    "    #print(sum(v), sum(ev))\n",
    "    print(\"The variances are: \", end=\" \")\n",
    "    for i in v:\n",
    "        print(f\"{i:.3f}\", end=\" \")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"The explained variances after PCA are: \", end=\" \")\n",
    "    for i in ev:\n",
    "        print(f\"{i:.3f}\", end=\" \")\n",
    "\n",
    "    plt.plot(np.arange(1,11), np.cumsum(ev))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary (week 6)\n",
    "\n",
    "* We got to know another supervised learning method, namely, naive Bayes classification\n",
    "* We saw examples of naive Bayes classification where either Gaussian or multinomial distribution was used to model the features of samples belonging to a class\n",
    "* We saw how to use cross validation to asses prediction abilities of a model. This allows us to be sure that the model is not overfitting.\n",
    "* In the clustering section we saw examples of using k-means, DBSCAN, and hierarchical clustering methods. They have different approaches to clustering, and each have different strengths.\n",
    "* Clustering is based on the notion of distance between the points in the data.\n",
    "* Principal component analysis is another example of unsupervised learning\n",
    "* It can reduce the dimensionality of a data by throwing away those dimensions where the variability is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/saskeli/x/blob/master/pca.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
