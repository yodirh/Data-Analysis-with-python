{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/saskeli/x/blob/master/clustering.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n",
    "\n",
    "|                                     -                                     |                                     -                                     |                                     -                                     |\n",
    "|---------------------------------------------------------------------------|---------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "|   [Exercise 5 (plant clustering)](<#Exercise-5-(plant-clustering&#41;>)   | [Exercise 6 (nonconvex clusters)](<#Exercise-6-(nonconvex-clusters&#41;>) |      [Exercise 7 (binding sites)](<#Exercise-7-(binding-sites&#41;>)      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is one of the types of unsupervised learning. It is similar to classification: the aim is to give a label to each data point. However, unlike in classification, we are not given any examples of labels associated with the data points. We must infer from the data, which data points belong to the same cluster. This can be achieved using some notion of distance between the data points. Data points in the same cluster are somehow close to each other.\n",
    "\n",
    "One of the simplest clustering methods is the *k-means clustering*. It aims at producing a clustering that is optimal in the following sense:\n",
    "\n",
    "* the *centre of each cluster* is the average of all points in the cluster\n",
    "* any point in a cluster is closer to its centre than to a centre of any other cluster\n",
    "\n",
    "The k-means clustering is first given the wanted number of clusters, say k, as a *hyperparameter*. Next, to start the algorithm, k points from the data set are chosen randomly as cluster centres. Then the following phases are repeated iteratively:\n",
    "\n",
    "* any data point is set to belong to a cluster, whose centre is closest to it\n",
    "* then for each cluster a new centre is chosen as the average of the data points in the cluster\n",
    "\n",
    "This procedure is repeated until the clusters no longer change. This kind of algorithm is called an Expectation-Maximization (EM) algorithm, which is known to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div class=\"alert alert-info\">Exercise 5 (plant clustering)</div>\n",
    "\n",
    "Using the same iris data set that you saw earlier in the classification, apply k-means clustering with 3 clusters.\n",
    "Create a function `plant_clustering` that loads the iris data set, clusters the data and returns the accuracy_score.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from itertools import permutations\n",
    "import scipy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def find_permutation(n_clusters, real_labels, labels):\n",
    "    permutation=[]\n",
    "    for i in range(n_clusters):\n",
    "        idx = labels == i\n",
    "        # Choose the most common label among data points in the cluster\n",
    "        new_label=scipy.stats.mode(real_labels[idx])[0][0]\n",
    "        permutation.append(new_label)\n",
    "    return permutation\n",
    "\n",
    "def plant_clustering():\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "    model = KMeans(3, random_state= 0)\n",
    "    model.fit(X)\n",
    "    permutation = find_permutation(3, y, model.labels_)\n",
    "    new_labels = [permutation[label] for label in model.labels_]\n",
    "\n",
    "    return accuracy_score(y, new_labels)\n",
    "\n",
    "def main():\n",
    "    print(plant_clustering())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div class=\"alert alert-info\">Exercise 6 (nonconvex clusters)</div>\n",
    "\n",
    "This exercise can give four points at maximum!\n",
    "\n",
    "Read the tab separated file data.tsv from the `src` folder into a DataFrame. The dataset has two features X1 and X2, and the label y. Cluster the feature matrix using DBSCAN with different values for the eps parameter. Use values in `np.arange(0.05, 0.2, 0.05)` for clustering. For each clustering, collect the accuracy score, the number of clusters, and the number of outliers. Return these values in a DataFrame, where columns and column names are as in the below example.\n",
    "\n",
    "Note that DBSCAN uses label -1 to denote outliers , that is, those data points that didn't fit well in any cluster. You have to modify the find_permutation function to handle this: ignore the outlier data points from the accuracy score computation. In addition, if the number of clusters is not the same as the number of labels in the original data, set the accuracy score to NaN.\n",
    "\n",
    "         eps   Score  Clusters  Outliers                             \n",
    "    0    0.05      ?         ?         ?\n",
    "    1    0.10      ?         ?         ?\n",
    "    2    0.15      ?         ?         ?\n",
    "    3    0.20      ?         ?         ?\n",
    "\n",
    "Before submitting the solution, you can plot the data set (with clusters colored) to see what kind of data we are dealing with.\n",
    "\n",
    "Points are given for each correct column in the result DataFrame.\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_permutation(n_clusters, real_labels, labels):\n",
    "    permutation=[]\n",
    "    for i in range(n_clusters):\n",
    "        idx = labels == i\n",
    "        # Choose the most common label among data points in the cluster\n",
    "        new_label=scipy.stats.mode(real_labels[idx])[0][0]\n",
    "        permutation.append(new_label)\n",
    "    return permutation\n",
    "\n",
    "def nonconvex_clusters():\n",
    "    df = pd.read_csv(\"src/data.tsv\", sep = \"\\t\")\n",
    "    X= df[[\"X1\", \"X2\"]]\n",
    "    y= df[\"y\"]\n",
    "    label_count = len(y.unique())\n",
    "    eps = np.arange(0.05,0.2,0.05)\n",
    "    \n",
    "    final = []\n",
    "\n",
    "    for i, val in enumerate(eps):\n",
    "        model = DBSCAN(eps=val)\n",
    "        model.fit(X)\n",
    "        cluster_count = len(set(model.labels_)) - (1 if -1 in model.labels_ else 0)\n",
    "        outlier_count = np.count_nonzero(model.labels_ == -1)\n",
    "        non_outliers = model.labels_ !=-1\n",
    "        permutation = find_permutation(cluster_count, y[non_outliers], model.labels_[non_outliers])\n",
    "        new_labels = [permutation[label] for label in model.labels_[non_outliers]]\n",
    "        if label_count != cluster_count:\n",
    "            score = np.nan\n",
    "        else:\n",
    "            score= accuracy_score(y[non_outliers], new_labels)\n",
    "\n",
    "        \n",
    "        final.append([val, score, cluster_count, outlier_count])\n",
    "\n",
    "    plt.scatter(X[\"X1\"], X[\"X2\"],c= model.labels_)\n",
    "    plt.show()\n",
    "    return pd.DataFrame(data=final, columns = [\"eps\", \"Score\", \"Clusters\", \"Outliers\"], dtype=\"float\")\n",
    "\n",
    "def main():\n",
    "    print(nonconvex_clusters())\n",
    "    #x= nonconvex_clusters()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div class=\"alert alert-info\">Exercise 7 (binding sites)</div>\n",
    "\n",
    "This exercise can give three points at maximum!\n",
    "\n",
    "A binding site is a piece of DNA where a certain protein prefers to bind. The piece of DNA can be described as a string consisting of letters A, C, G, and T, which correspond to nucleotides Adenine, Cytosine, Guanine, and Thymine.\n",
    "In this exercise the length of binding sites is eight nucleotides. They are stored in the file `data.seq`,\n",
    "and the binding sites there are classified into two classes.\n",
    "\n",
    "Part 1. Write function `toint` that converts a nucleotide to an integer. Use the following mapping:\n",
    "```\n",
    "A -> 0\n",
    "C -> 1\n",
    "G -> 2\n",
    "T -> 3\n",
    "```\n",
    "\n",
    "Write also function `get_features_and_labels` that gets a filename as a parameter. The function should load the contents of the file into a DataFrame. The column `X` contains a string. Convert this column into a feature matrix using the above `toint` function. For example the column `[\"GGATAATA\",\"CGATAACC\"]` should result to the feature matrix\n",
    "```\n",
    "[[2,2,0,3,0,0,3,0],\n",
    "[1,2,0,3,0,0,1,1]]\n",
    "```\n",
    "The function should return a pair, whose first element is the feature matrix and the second element is the label vector.\n",
    "\n",
    "Part 2. Create function `cluster_euclidean` that gets a filename as parameter. Get the features and labels using the function from part 1. Perform hierarchical clustering using the function `sklearn.cluster.AgglomerativeClustering`. Get two clusters using `average` linkage and `euclidean` affinity. Fit the model and predict the labels. Note that you may have to use the `find_permutation` function again, because even though the clusters are correct, they may be labeled differently than the real labels given in `data.seq`. The function should return the accuracy score.\n",
    "\n",
    "Part 3. Create function `cluster_hamming` that works like the function in part 2, except now using the [hamming](https://en.wikipedia.org/wiki/Hamming_distance) affinity. Even though it is possible to pass the function `hamming` to `AgglomerativeClustering`, let us now compute the Hamming distance matrix explicitly. We can achieve this using the function `sklearn.metrics.pairwise_distances`. Use the affinity parameter `precomputed` to `AgglomerativeClustering`. And give the distance matrix you got from `pairwise_distances`, instead of the feature matrix, to the `fit_predict` method of the model. If you want, you can visualize the clustering using the provided `plot` function.\n",
    "\n",
    "**NB!** When submitting your solution, please comment away all `plot` function calls. This might cause tests to fail on the server.\n",
    "\n",
    "Which affinity (or distance) do you think is theoretically more correct of these two (Euclidean or Hamming)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from numpy.core.function_base import linspace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import scipy.spatial as sp\n",
    "import scipy.cluster.hierarchy as hc\n",
    "import scipy\n",
    "\n",
    "def find_permutation(n_clusters, real_labels, labels):\n",
    "    permutation=[]\n",
    "    for i in range(n_clusters):\n",
    "        idx = labels == i\n",
    "        # Choose the most common label among data points in the cluster\n",
    "        new_label=scipy.stats.mode(real_labels[idx])[0][0]\n",
    "        permutation.append(new_label)\n",
    "    return permutation\n",
    "\n",
    "def toint(x):\n",
    "    empty = []\n",
    "    for c in x:\n",
    "        if c ==\"A\":\n",
    "            empty.append(0)\n",
    "        if c ==\"C\":\n",
    "            empty.append(1)\n",
    "        if c ==\"G\":\n",
    "            empty.append(2)\n",
    "        if c ==\"T\":\n",
    "            empty.append(3)\n",
    "    #print(empty)\n",
    "    return empty\n",
    "\n",
    "def get_features_and_labels(filename):\n",
    "    df = pd.read_csv(filename, sep =\"\\t\")\n",
    "    empty= []\n",
    "    for c in df[\"X\"]:\n",
    "            empty.append(toint(c))\n",
    "    #X= [c for c in df[\"X\"]]\n",
    "    return (np.array(empty), np.array(df[\"y\"]))\n",
    "\n",
    "\n",
    "def plot(distances, method='average', affinity='euclidean'):\n",
    "    mylinkage = hc.linkage(sp.distance.squareform(distances), method=method)\n",
    "    g=sns.clustermap(distances, row_linkage=mylinkage, col_linkage=mylinkage )\n",
    "    g.fig.suptitle(f\"Hierarchical clustering using {method} linkage and {affinity} affinity\")\n",
    "    plt.show()\n",
    "\n",
    "def cluster_euclidean(filename):\n",
    "    X, y = get_features_and_labels(filename)\n",
    "    model = AgglomerativeClustering(affinity=\"euclidean\" ,linkage=\"average\")\n",
    "    model.fit(X)\n",
    "    permutation = find_permutation(2, y,model.labels_)\n",
    "    new_labels= [permutation[label] for label in model.labels_]\n",
    "    return accuracy_score(y,new_labels)\n",
    "\n",
    "def cluster_hamming(filename):\n",
    "    X, y = get_features_and_labels(filename)\n",
    "    dist = pairwise_distances(X, metric=\"hamming\")\n",
    "    model = AgglomerativeClustering(affinity=\"precomputed\", linkage=\"average\")\n",
    "    model.fit_predict(dist)\n",
    "    permutation = find_permutation(2, y,model.labels_)\n",
    "    new_labels= [permutation[label] for label in model.labels_]\n",
    "    return accuracy_score(y,new_labels)\n",
    "\n",
    "\n",
    "def main():\n",
    "    #print(toint(\"A\"))\n",
    "    #print(get_features_and_labels(\"src/data.seq\"))\n",
    "    print(\"Accuracy score with Euclidean affinity is\", cluster_euclidean(\"src/data.seq\"))\n",
    "    print(\"Accuracy score with Hamming affinity is\", cluster_hamming(\"src/data.seq\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/saskeli/x/blob/master/clustering.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
